{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6  148  72  35    0  33.6  0.627  50  1\n",
       "0  1   85  66  29    0  26.6  0.351  31  0\n",
       "1  8  183  64   0    0  23.3  0.672  32  1\n",
       "2  1   89  66  23   94  28.1  0.167  21  0\n",
       "3  0  137  40  35  168  43.1  2.288  33  1\n",
       "4  5  116  74   0    0  25.6  0.201  30  0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('pima-indians-diabetes.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense(10, init=\"normal\", activation=\"relu\").\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 0s 819us/step - loss: 3.3176 - accuracy: 0.5182\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 1.4759 - accuracy: 0.6354\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 838us/step - loss: 0.8601 - accuracy: 0.6393\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 842us/step - loss: 0.7256 - accuracy: 0.6576\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.6700 - accuracy: 0.6576\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 790us/step - loss: 0.6369 - accuracy: 0.6693\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6125 - accuracy: 0.6732\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.6197 - accuracy: 0.6654\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.6095 - accuracy: 0.6719\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.6051 - accuracy: 0.6771\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 803us/step - loss: 0.6013 - accuracy: 0.6719\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 803us/step - loss: 0.6019 - accuracy: 0.6758\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.5977 - accuracy: 0.6849\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.5979 - accuracy: 0.6849\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.6012 - accuracy: 0.6784\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5914 - accuracy: 0.6875\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.5906 - accuracy: 0.6927\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 907us/step - loss: 0.5895 - accuracy: 0.6953\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 920us/step - loss: 0.5863 - accuracy: 0.6966\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.5845 - accuracy: 0.7018\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.5908 - accuracy: 0.6888\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.5792 - accuracy: 0.7070\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 842us/step - loss: 0.5792 - accuracy: 0.7057\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5771 - accuracy: 0.7083\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5734 - accuracy: 0.6927\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.5799 - accuracy: 0.6953\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.5770 - accuracy: 0.6979\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.5725 - accuracy: 0.6979\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5748 - accuracy: 0.6979\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.5677 - accuracy: 0.7031\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 790us/step - loss: 0.5744 - accuracy: 0.6992\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.5673 - accuracy: 0.7122\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 687us/step - loss: 0.5711 - accuracy: 0.6992\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5697 - accuracy: 0.7044\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 661us/step - loss: 0.5640 - accuracy: 0.7148\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.5677 - accuracy: 0.6992\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 803us/step - loss: 0.5643 - accuracy: 0.7214\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 648us/step - loss: 0.5745 - accuracy: 0.7057\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.5576 - accuracy: 0.7201\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 648us/step - loss: 0.5639 - accuracy: 0.6992\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5600 - accuracy: 0.7161\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5560 - accuracy: 0.7253\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5553 - accuracy: 0.7188\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.5619 - accuracy: 0.7083\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5553 - accuracy: 0.7096\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5557 - accuracy: 0.7083\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 868us/step - loss: 0.5621 - accuracy: 0.7031\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5551 - accuracy: 0.7148\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.5470 - accuracy: 0.7174\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.5478 - accuracy: 0.7227\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5523 - accuracy: 0.7214\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5518 - accuracy: 0.7188\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 635us/step - loss: 0.5465 - accuracy: 0.7135\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 648us/step - loss: 0.5459 - accuracy: 0.7201\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5500 - accuracy: 0.7240\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 920us/step - loss: 0.5398 - accuracy: 0.7396\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5448 - accuracy: 0.7409\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5385 - accuracy: 0.7435\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5408 - accuracy: 0.7318\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 881us/step - loss: 0.5369 - accuracy: 0.7396\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.5367 - accuracy: 0.7474\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7474\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 894us/step - loss: 0.5379 - accuracy: 0.7331\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 920us/step - loss: 0.5310 - accuracy: 0.7435\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.5281 - accuracy: 0.7682\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5257 - accuracy: 0.7500\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 790us/step - loss: 0.5254 - accuracy: 0.7487\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.5258 - accuracy: 0.7422\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.5201 - accuracy: 0.7526\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.5197 - accuracy: 0.7565\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.5249 - accuracy: 0.7435\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.5256 - accuracy: 0.7578\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7552\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 984us/step - loss: 0.5188 - accuracy: 0.7422\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 868us/step - loss: 0.5215 - accuracy: 0.7500\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 881us/step - loss: 0.5225 - accuracy: 0.7487\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 894us/step - loss: 0.5274 - accuracy: 0.7526\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 907us/step - loss: 0.5248 - accuracy: 0.7461\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5164 - accuracy: 0.7474\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 868us/step - loss: 0.5174 - accuracy: 0.7409\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 920us/step - loss: 0.5259 - accuracy: 0.7435\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 842us/step - loss: 0.5114 - accuracy: 0.7539\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 881us/step - loss: 0.5212 - accuracy: 0.7552\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 920us/step - loss: 0.5188 - accuracy: 0.7513\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.5164 - accuracy: 0.7591\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 894us/step - loss: 0.5137 - accuracy: 0.7461\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.5147 - accuracy: 0.7526\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 842us/step - loss: 0.5170 - accuracy: 0.7500\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 803us/step - loss: 0.5074 - accuracy: 0.7487\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 803us/step - loss: 0.5090 - accuracy: 0.7474\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 803us/step - loss: 0.5034 - accuracy: 0.7643\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.5081 - accuracy: 0.7526\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.5090 - accuracy: 0.7604\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.5184 - accuracy: 0.7565\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.5031 - accuracy: 0.7617\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5162 - accuracy: 0.7526\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7474\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.5031 - accuracy: 0.7526\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.5031 - accuracy: 0.7565\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.4969 - accuracy: 0.7695\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7578\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4974 - accuracy: 0.7487\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.5022 - accuracy: 0.7591\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.5043 - accuracy: 0.7474\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.4987 - accuracy: 0.7682\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.4998 - accuracy: 0.7487\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.5014 - accuracy: 0.7669\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.4999 - accuracy: 0.7474\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4930 - accuracy: 0.7578\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.5020 - accuracy: 0.7513\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.4957 - accuracy: 0.7721\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.4919 - accuracy: 0.7656\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.4970 - accuracy: 0.7578\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.4965 - accuracy: 0.7513\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.4990 - accuracy: 0.7591\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 816us/step - loss: 0.4928 - accuracy: 0.7578\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.4866 - accuracy: 0.7656\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4871 - accuracy: 0.7565\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4992 - accuracy: 0.7604\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 751us/step - loss: 0.4957 - accuracy: 0.7643\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 764us/step - loss: 0.4895 - accuracy: 0.7565\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.4930 - accuracy: 0.7578\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.4897 - accuracy: 0.7630\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 713us/step - loss: 0.4902 - accuracy: 0.7552\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.4869 - accuracy: 0.7695\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4901 - accuracy: 0.7539\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4812 - accuracy: 0.7721\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.4827 - accuracy: 0.7669\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.5023 - accuracy: 0.7617\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4856 - accuracy: 0.7552\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 855us/step - loss: 0.4892 - accuracy: 0.7643\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.4844 - accuracy: 0.7617\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 699us/step - loss: 0.4797 - accuracy: 0.7760\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.4823 - accuracy: 0.7656\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.4949 - accuracy: 0.7565\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.4826 - accuracy: 0.7734\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 692us/step - loss: 0.4771 - accuracy: 0.7578\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 894us/step - loss: 0.4792 - accuracy: 0.7643\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.7656\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4750 - accuracy: 0.7799\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.4797 - accuracy: 0.7604\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.4838 - accuracy: 0.7773\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 738us/step - loss: 0.4891 - accuracy: 0.7617\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 777us/step - loss: 0.4732 - accuracy: 0.7773\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.4763 - accuracy: 0.7604\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 712us/step - loss: 0.4696 - accuracy: 0.7643\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 725us/step - loss: 0.4800 - accuracy: 0.7682\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 674us/step - loss: 0.4772 - accuracy: 0.7734\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 686us/step - loss: 0.4679 - accuracy: 0.7799\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 829us/step - loss: 0.4707 - accuracy: 0.7773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d898aa65b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 790us/step - loss: 0.4625 - accuracy: 0.7812\n",
      "accuracy: 78.12%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Verification in Keras using Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed=7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 12.2488 - accuracy: 0.3833 - val_loss: 5.1722 - val_accuracy: 0.5276\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 2.8953 - accuracy: 0.5350 - val_loss: 1.8728 - val_accuracy: 0.4961\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.2792 - accuracy: 0.5856 - val_loss: 1.4110 - val_accuracy: 0.6260\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 1.1033 - accuracy: 0.6323 - val_loss: 1.1820 - val_accuracy: 0.6260\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.9412 - accuracy: 0.6479 - val_loss: 1.0348 - val_accuracy: 0.6417\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.8576 - accuracy: 0.6459 - val_loss: 0.9755 - val_accuracy: 0.5827\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7899 - accuracy: 0.6693 - val_loss: 0.8636 - val_accuracy: 0.6181\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7350 - accuracy: 0.6848 - val_loss: 0.8364 - val_accuracy: 0.6457\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.7058 - accuracy: 0.6693 - val_loss: 0.7664 - val_accuracy: 0.6693\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6709 - accuracy: 0.6848 - val_loss: 0.8255 - val_accuracy: 0.6811\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6482 - accuracy: 0.6984 - val_loss: 0.7054 - val_accuracy: 0.6654\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6296 - accuracy: 0.6946 - val_loss: 0.8580 - val_accuracy: 0.6772\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6338 - accuracy: 0.6926 - val_loss: 0.6969 - val_accuracy: 0.7008\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6237 - accuracy: 0.6926 - val_loss: 0.6609 - val_accuracy: 0.6693\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6160 - accuracy: 0.6984 - val_loss: 0.7374 - val_accuracy: 0.6220\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6923 - accuracy: 0.6926 - val_loss: 0.6457 - val_accuracy: 0.6772\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6141 - accuracy: 0.6887 - val_loss: 0.6413 - val_accuracy: 0.6693\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6083 - accuracy: 0.6984 - val_loss: 0.7896 - val_accuracy: 0.6654\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6502 - accuracy: 0.6809 - val_loss: 0.6452 - val_accuracy: 0.6929\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5808 - accuracy: 0.7062 - val_loss: 0.6554 - val_accuracy: 0.6850\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5933 - accuracy: 0.7179 - val_loss: 0.6939 - val_accuracy: 0.6417\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.7082 - val_loss: 0.6521 - val_accuracy: 0.6929\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.6868 - val_loss: 0.8055 - val_accuracy: 0.6732\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5913 - accuracy: 0.7121 - val_loss: 0.6213 - val_accuracy: 0.6929\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6331 - accuracy: 0.7023 - val_loss: 0.6181 - val_accuracy: 0.7008\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5834 - accuracy: 0.7140 - val_loss: 0.6228 - val_accuracy: 0.6850\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5669 - accuracy: 0.7121 - val_loss: 0.6175 - val_accuracy: 0.7008\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7140 - val_loss: 0.6086 - val_accuracy: 0.6929\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5966 - accuracy: 0.7004 - val_loss: 0.6119 - val_accuracy: 0.7008\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6021 - accuracy: 0.6946 - val_loss: 0.6486 - val_accuracy: 0.6732\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.6105 - accuracy: 0.7004 - val_loss: 0.6065 - val_accuracy: 0.6929\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.7082 - val_loss: 0.6135 - val_accuracy: 0.7008\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7276 - val_loss: 0.7626 - val_accuracy: 0.6811\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5760 - accuracy: 0.7179 - val_loss: 0.6078 - val_accuracy: 0.6614\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5618 - accuracy: 0.7121 - val_loss: 0.6258 - val_accuracy: 0.6535\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5455 - accuracy: 0.7393 - val_loss: 0.6344 - val_accuracy: 0.6772\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5921 - accuracy: 0.6984 - val_loss: 0.5963 - val_accuracy: 0.7008\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5641 - accuracy: 0.7101 - val_loss: 0.6079 - val_accuracy: 0.6732\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5796 - accuracy: 0.7160 - val_loss: 0.5954 - val_accuracy: 0.6929\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5544 - accuracy: 0.7257 - val_loss: 0.6952 - val_accuracy: 0.6811\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.7043 - val_loss: 0.7289 - val_accuracy: 0.6772\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5935 - accuracy: 0.7043 - val_loss: 0.6670 - val_accuracy: 0.6772\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5540 - accuracy: 0.7257 - val_loss: 0.6553 - val_accuracy: 0.6890\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5664 - accuracy: 0.7218 - val_loss: 0.5895 - val_accuracy: 0.6929\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.6984 - val_loss: 0.6756 - val_accuracy: 0.6575\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5888 - accuracy: 0.7062 - val_loss: 0.5940 - val_accuracy: 0.6969\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5496 - accuracy: 0.7198 - val_loss: 0.6194 - val_accuracy: 0.6890\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5648 - accuracy: 0.7140 - val_loss: 0.6539 - val_accuracy: 0.6969\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5921 - accuracy: 0.7062 - val_loss: 0.6693 - val_accuracy: 0.7047\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5531 - accuracy: 0.7315 - val_loss: 0.5876 - val_accuracy: 0.7165\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5603 - accuracy: 0.7335 - val_loss: 0.5872 - val_accuracy: 0.7126\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5468 - accuracy: 0.7335 - val_loss: 0.6595 - val_accuracy: 0.6850\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5422 - accuracy: 0.7218 - val_loss: 0.6256 - val_accuracy: 0.6811\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5481 - accuracy: 0.7140 - val_loss: 0.5955 - val_accuracy: 0.7126\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5447 - accuracy: 0.7237 - val_loss: 0.6185 - val_accuracy: 0.6890\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.7276 - val_loss: 0.5837 - val_accuracy: 0.6969\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5559 - accuracy: 0.7082 - val_loss: 0.6175 - val_accuracy: 0.6929\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5375 - accuracy: 0.7315 - val_loss: 0.6004 - val_accuracy: 0.6969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5415 - accuracy: 0.7412 - val_loss: 0.5789 - val_accuracy: 0.7244\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5300 - accuracy: 0.7490 - val_loss: 0.5987 - val_accuracy: 0.6890\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7198 - val_loss: 0.6121 - val_accuracy: 0.7008\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5621 - accuracy: 0.7218 - val_loss: 0.6638 - val_accuracy: 0.6890\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7160 - val_loss: 0.6567 - val_accuracy: 0.7047\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.7354 - val_loss: 0.5869 - val_accuracy: 0.7165\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7412 - val_loss: 0.5887 - val_accuracy: 0.6929\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.7276 - val_loss: 0.6298 - val_accuracy: 0.6969\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5302 - accuracy: 0.7451 - val_loss: 0.5883 - val_accuracy: 0.6929\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7412 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.7393 - val_loss: 0.6852 - val_accuracy: 0.6811\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5497 - accuracy: 0.7198 - val_loss: 0.5866 - val_accuracy: 0.7008\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7529 - val_loss: 0.6372 - val_accuracy: 0.6890\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.7315 - val_loss: 0.5856 - val_accuracy: 0.7205\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.7490 - val_loss: 0.5747 - val_accuracy: 0.6969\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7471 - val_loss: 0.5759 - val_accuracy: 0.7165\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5252 - accuracy: 0.7412 - val_loss: 0.5713 - val_accuracy: 0.7205\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.7451 - val_loss: 0.5753 - val_accuracy: 0.7087\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5137 - accuracy: 0.7549 - val_loss: 0.5752 - val_accuracy: 0.7165\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5341 - accuracy: 0.7315 - val_loss: 0.5721 - val_accuracy: 0.7008\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.7062 - val_loss: 0.6162 - val_accuracy: 0.6850\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5453 - accuracy: 0.7218 - val_loss: 0.6047 - val_accuracy: 0.7047\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7276 - val_loss: 0.5951 - val_accuracy: 0.7047\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7665 - val_loss: 0.5863 - val_accuracy: 0.7008\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7588 - val_loss: 0.5656 - val_accuracy: 0.7402\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5396 - accuracy: 0.7471 - val_loss: 0.6214 - val_accuracy: 0.7087\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7101 - val_loss: 0.5678 - val_accuracy: 0.7559\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5262 - accuracy: 0.7354 - val_loss: 0.5808 - val_accuracy: 0.7008\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5214 - accuracy: 0.7412 - val_loss: 0.5998 - val_accuracy: 0.7087\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5445 - accuracy: 0.7490 - val_loss: 0.5761 - val_accuracy: 0.7205\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5364 - accuracy: 0.7198 - val_loss: 0.5806 - val_accuracy: 0.7165\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5424 - accuracy: 0.7198 - val_loss: 0.5882 - val_accuracy: 0.7283\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5160 - accuracy: 0.7315 - val_loss: 0.5952 - val_accuracy: 0.7126\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5368 - accuracy: 0.7140 - val_loss: 0.5960 - val_accuracy: 0.7087\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5407 - accuracy: 0.7412 - val_loss: 0.5732 - val_accuracy: 0.7087\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.7471 - val_loss: 0.5642 - val_accuracy: 0.7126\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5278 - accuracy: 0.7412 - val_loss: 0.5755 - val_accuracy: 0.7283\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5058 - accuracy: 0.7490 - val_loss: 0.5630 - val_accuracy: 0.7126\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7296 - val_loss: 0.5854 - val_accuracy: 0.7205\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.7412 - val_loss: 0.5580 - val_accuracy: 0.7520\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7315 - val_loss: 0.5689 - val_accuracy: 0.7244\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5338 - accuracy: 0.7510 - val_loss: 0.5685 - val_accuracy: 0.7244\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5094 - accuracy: 0.7568 - val_loss: 0.5659 - val_accuracy: 0.7323\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7374 - val_loss: 0.5637 - val_accuracy: 0.7244\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.7607 - val_loss: 0.5726 - val_accuracy: 0.7244\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5072 - accuracy: 0.7529 - val_loss: 0.5647 - val_accuracy: 0.7441\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5109 - accuracy: 0.7568 - val_loss: 0.5521 - val_accuracy: 0.7480\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7588 - val_loss: 0.6458 - val_accuracy: 0.7047\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.7471 - val_loss: 0.6028 - val_accuracy: 0.6969\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4988 - accuracy: 0.7665 - val_loss: 0.5578 - val_accuracy: 0.7441\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7607 - val_loss: 0.5701 - val_accuracy: 0.7047\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7588 - val_loss: 0.5609 - val_accuracy: 0.7559\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7549 - val_loss: 0.5744 - val_accuracy: 0.7165\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.7490 - val_loss: 0.5818 - val_accuracy: 0.7205\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7549 - val_loss: 0.5736 - val_accuracy: 0.7205\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5620 - accuracy: 0.7354 - val_loss: 0.5529 - val_accuracy: 0.7283\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7237 - val_loss: 0.5567 - val_accuracy: 0.7441\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.7646 - val_loss: 0.5725 - val_accuracy: 0.7087\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5324 - accuracy: 0.7198 - val_loss: 0.5784 - val_accuracy: 0.7323\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5050 - accuracy: 0.7646 - val_loss: 0.5508 - val_accuracy: 0.7480\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.7510 - val_loss: 0.5538 - val_accuracy: 0.7480\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5089 - accuracy: 0.7490 - val_loss: 0.5726 - val_accuracy: 0.7126\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.7198 - val_loss: 0.5841 - val_accuracy: 0.7323\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.7315 - val_loss: 0.5491 - val_accuracy: 0.7520\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5203 - accuracy: 0.7568 - val_loss: 0.5562 - val_accuracy: 0.7402\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5417 - accuracy: 0.7354 - val_loss: 0.5493 - val_accuracy: 0.7559\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5206 - accuracy: 0.7335 - val_loss: 0.6111 - val_accuracy: 0.6929\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7471 - val_loss: 0.6004 - val_accuracy: 0.6969\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4906 - accuracy: 0.7646 - val_loss: 0.5653 - val_accuracy: 0.7205\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7432 - val_loss: 0.5452 - val_accuracy: 0.7480\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5007 - accuracy: 0.7529 - val_loss: 0.6087 - val_accuracy: 0.7126\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7529 - val_loss: 0.5490 - val_accuracy: 0.7520\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7743 - val_loss: 0.5960 - val_accuracy: 0.7244\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7626 - val_loss: 0.5422 - val_accuracy: 0.7559\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.7568 - val_loss: 0.5553 - val_accuracy: 0.7165\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7510 - val_loss: 0.5443 - val_accuracy: 0.7677\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7549 - val_loss: 0.5468 - val_accuracy: 0.7559\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4939 - accuracy: 0.7529 - val_loss: 0.5460 - val_accuracy: 0.7559\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4803 - accuracy: 0.7704 - val_loss: 0.5641 - val_accuracy: 0.7165\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4870 - accuracy: 0.7665 - val_loss: 0.5603 - val_accuracy: 0.7244\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5020 - accuracy: 0.7568 - val_loss: 0.5442 - val_accuracy: 0.7402\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7665 - val_loss: 0.5878 - val_accuracy: 0.7087\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7549 - val_loss: 0.5506 - val_accuracy: 0.7559\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7549 - val_loss: 0.5632 - val_accuracy: 0.7402\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7529 - val_loss: 0.6470 - val_accuracy: 0.7008\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5165 - accuracy: 0.7315 - val_loss: 0.5629 - val_accuracy: 0.7244\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.7315 - val_loss: 0.5446 - val_accuracy: 0.7441\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5143 - accuracy: 0.7588 - val_loss: 0.5631 - val_accuracy: 0.7362\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.7821 - val_loss: 0.6115 - val_accuracy: 0.7126\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.7471 - val_loss: 0.5430 - val_accuracy: 0.7441\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.7704 - val_loss: 0.5444 - val_accuracy: 0.7402\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4892 - accuracy: 0.7782 - val_loss: 0.5845 - val_accuracy: 0.7165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d89bbbcc10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual K Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#y=Y\n",
    "# define 10-fold cross validation test harness\n",
    "#StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "# folds=list(kf.split(x_train,y_train)) \n",
    "#kfold.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds=kf.split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89D078D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 70.13%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D8A11BE1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 63.64%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89E56EA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 68.83%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89CFFF8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 64.94%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D898B2B040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 71.43%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89FBFD940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 64.94%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89BB4A9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 71.43%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89E6E7280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 76.62%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D8A128E5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 67.11%\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001D89E9869D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "accuracy: 77.63%\n",
      "69.67% (+/- 4.55%)\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(X, Y):\n",
    "  # create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# Fit the model\n",
    "\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "\t# evaluate the model\n",
    "\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\tcvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
